## 
# This script runs a series of experiments to evaluate the creativity and clinical usefulness of biomedical ideas generated by various LLMs.
# It uses OpenAI's Azure API to call models, judge the generated ideas, and analyze the results.
# The script is structured to allow for easy configuration of models, tasks, and parameters.
# It collects results in a structured format and outputs both raw data and summary statistics.
##

## 
# Creator: Ryan Mehra
# Date: 2024-07-10
# License: MIT License
##

import os
import re
import json
import time
import random
from collections import defaultdict, Counter

import pandas as pd

import openai
import json

from langchain_openai import AzureChatOpenAI

openai.api_version = "2025-03-01-preview"

# #####################################################################
# 0. CONFIGURATION SECTION                                           #
#####################################################################

# ---- Models to test (rename if your actual deployment names differ) ----
GEN_MODELS = [
    "gpt-4o",
    "gpt-4o-mini",
    "gpt-4.1",        
    "gpt-4.1-mini",  
    # Optional DeepSeek models – include if you have wrappers configured
    # "deepseek-r1-50b",
    # "deepseek-r1-8b",
]

JUDGE_MODEL = "gpt-4o"          # deterministic judge (temperature=0)

REPLICATES = 3                  # number of independent runs per (model,condition,task)
RANDOM_SEED = 42                # for reproducibility
random.seed(RANDOM_SEED)

# ---- Prompt templates ----------------------------------------------
SYS_BASELINE = (
    "You are a meticulous medical research assistant. Provide ideas grounded in "
    "peer‑reviewed evidence. Do NOT speculate beyond currently validated data."
)
SYS_CREATIVE = (
    "You are an imaginative biomedical inventor. Bold, speculative ideas are welcome "
    "— even if unverified. Label any made‑up details as speculative."
)

# Tasks: (task_id, user_prompt)
TASKS = [
    (
        "T1",
        "Generate **five** fresh, unconventional hypotheses for the pathogenesis of "
        "Alzheimer’s disease. Provide a one‑sentence rationale for each. Number your list 1‑5.\n\n"
        "Return ONLY a numbered list. No opening prose, no closing remarks."
    ),
    (
        "T2",
        "Propose **five** innovative therapeutic approaches to combat multi‑drug‑resistant "
        "bacterial infections. Each idea ≤75 words. Number 1‑5.\n\n"
        "Return ONLY a numbered list."
    ),
    (
        "T3",
        "Brainstorm **five** novel medical‑device concepts aimed at reducing hospital‑acquired "
        "infections. Briefly (≤60 words) describe how each device works. Number 1‑5.\n\n"
        "Return ONLY a numbered list."
    ),
]

BASELINE_PARAMS = dict(
    temperature=0.2,
    top_p=0.9,
    presence_penalty=0.0,
    frequency_penalty=0.0,
    max_tokens=400,
)

CREATIVE_PARAMS = dict(
    temperature=1.1,
    top_p=0.97,
    presence_penalty=1.0,
    frequency_penalty=0.3,
    max_tokens=600,
)

# ---- Judge prompt --------------------------------------------------
SYS_JUDGE = (
    "You are an expert evaluator of biomedical creativity. Rate the idea below "
    "for (1) Novelty and (2) Prospective Clinical Usefulness on a 0‑5 integer scale "
    "(0 = none, 5 = exceptional). Respond as strict JSON with keys "
    '"novelty", "usefulness", and "comment".'
)

JUDGE_PARAMS = dict(
    temperature=0, top_p=1.0, max_tokens=256
)

######################################################################
# 1. HELPER FUNCTIONS                                                 
######################################################################


def call_llm(model: str, sys_msg: str, user_msg: str, **kwargs) -> str:
    """
    Wrapper for a chat completion. Replace with your custom model call if needed.
    """

    llm = AzureChatOpenAI(deployment_name=model,
                      azure_endpoint = "<endpoint>", 
                      api_key="<api_key>",
                      api_version=openai.api_version,
                      temperature=kwargs.get("temperature", 0.2),
                      top_p=kwargs.get("top_p", 0.9),
                      presence_penalty=kwargs.get("presence_penalty", 0.0),
                      frequency_penalty=kwargs.get("frequency_penalty", 0.0),
                      max_tokens=kwargs.get("max_tokens", 500)
                    )
    
    messages=[
            {"role": "system", "content": sys_msg},
            {"role": "user", "content": user_msg},
        ]
    
    output = llm.invoke(messages).content
    return output


NUM_LIST_RE = re.compile(r"^\s*(\d+)[\).\s-]+", re.MULTILINE)


def split_numbered_list(text: str) -> list[str]:
    """
    Parse a 1‑N numbered list from model output.
    """
    lines = [ln.strip() for ln in text.strip().splitlines() if ln.strip()]
    ideas = []
    current = []
    for ln in lines:
        if NUM_LIST_RE.match(ln):
            if current:
                ideas.append(" ".join(current).strip())
                current = []
            ln = NUM_LIST_RE.sub("", ln, count=1).strip()
        current.append(ln)
    if current:
        ideas.append(" ".join(current).strip())
    return ideas


def judge_idea(idea: str) -> dict:
    """
    Call judge LLM and parse JSON rating.
    """

    llm = AzureChatOpenAI(deployment_name=JUDGE_MODEL,
                  azure_endpoint = "<endpoint>", 
                  api_key="<api_key>",
                  api_version=openai.api_version,
                  temperature=0,
                  top_p=1.0,
                  max_tokens=256,
                  model_kwargs=dict(
                      response_format={"type": "json_object"}
                  )
       )
    
    messages=[
            {"role": "system", "content": SYS_JUDGE},
            {"role": "user", "content": idea},
        ]
    output = llm.invoke(messages).content

    return json.loads(output)


######################################################################
# 2. MAIN EXPERIMENT LOOP                                             #
######################################################################

records = []

for model in GEN_MODELS:
    for condition in ["baseline", "creative"]:
        sys_msg = SYS_BASELINE if condition == "baseline" else SYS_CREATIVE
        params = BASELINE_PARAMS if condition == "baseline" else CREATIVE_PARAMS
        for task_id, user_prompt in TASKS:
            for rep in range(1, REPLICATES+2):
                try:
                    raw_output = call_llm(model, sys_msg, user_prompt, **params)
                except openai.BadRequestError as e:
                    print(f"Skipping replicate {rep} due to LLM error: {e}")
                    continue
                ideas = split_numbered_list(raw_output)
                # Ensure exactly 5 slots (pad or trim)
                ideas = ideas[:5] + [""] * (5 - len(ideas))
                print(f"\n\nIdeas: {ideas}")
                for idx, idea in enumerate(ideas, start=1):
                    if not idea:
                        continue
                    try:
                        rating = judge_idea(idea)
                    except openai.BadRequestError as e:
                        print(f"Skipping idea {idx} due to judge error: {e}")
                        continue
                    creativity_score = (
                         rating["novelty"] * rating["usefulness"] / 25.0
                     )  # 0‑1
                    records.append(
                         dict(
                             model=model,
                             condition=condition,
                             task=task_id,
                             replicate=rep,
                             idea_index=idx,
                             idea=idea,
                             novelty=rating["novelty"],
                             usefulness=rating["usefulness"],
                             creativity=creativity_score,
                             comment=rating.get("comment", ""),
                         )
                     )

######################################################################
# 3. ANALYSIS & REPORTING                                            #
######################################################################

df = pd.DataFrame.from_records(records)
if df.empty:
    print("No data collected (running in placeholder mode).")
else:
    df.to_csv("llm_hallucination_raw_ideas.csv", index=False)

    # Compute high‑value / noise metrics
    df["high_value"] = df["creativity"] >= 0.6
    df["noise"] = df["usefulness"] <= 1

    summary = (
        df.groupby(["model", "condition"])
        .agg(
            n_ideas=("idea", "count"),
            mean_creativity=("creativity", "mean"),
            pct_high_value=("high_value", "mean"),
            pct_noise=("noise", "mean"),
        )
        .reset_index()
    )
    summary.to_csv("llm_hallucination_summary.csv", index=False)

    # Show quick preview
    print("Summary of Creativity Scores:")
    print(summary)
